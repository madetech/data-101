# A typical Machine Learning Workflow
It is no secret that understanding data is the key to unlocking hidden potential across our Public Sector organisations. Be it through optimisations in analysis, the monitoring of services or identification of previously unknown trends, the importance of data is unquestionable. So what part do Made Tech play in this brave new data driven world? Well, as ever, our role is one of guidance, education and reliable implementation. We must ensure that we posses the ability to mentor this burgeoning capability within the public sector and the confidence to lead them through the data landscape. 

The goal of this document is to outline the framework for a typical Machine Learning (ML) project. We will briefly discuss each step and its importance, what factors must be considered and what realities our clients (and our data team's) are likely to face. We will cover how to identify business needs and how to frame problems as questions that can be answered by a machine learning model. We will explore the realities of developing a high quality data set and what is necessary to develop a robust ML solution. Core to this process, is how it will impact the business and how best we can demonstrate the value added to the client.

![A typical machine learning workflow](../images/ml-workflow.jpg)

Software development is always an iterative process, agile workflows and short sprint cycles are now well known and widely accepted as best practice within the public sector. Although this workflow is mirrored in the data world the nature of ML development can often make it seem like very little is ever being done. The process of model development is incredibly iterative and will produce very little in terms of tangible outputs until it is complete. The cycle of feature engineering and model training is in many ways far more similar to a tech spike, trying new techniques and models until something yields the desired result. Where a typical sprint may add a selection of new features to an app, a successful ML sprint may only see several new columns (data features) added to a database. It may take several weeks to prepare data and develop features, only to find that the resulting model does not reach the required metrics and the whole process must begin once again. 

The implementation of ML is difficult, it takes a lot of time and money and there will always be the risk that a project may run out of either of these before a successful model can be produced. This makes open and honest communication incredibly important with any stake holder. 

## Identify and Framing the Business Problem 
The first objective of any speculative data project is convincing stake holders of the need for a data strategy. Highlighting the myriad of benefits that accompany a mature data platform is an incredibly exciting process, as usually the development of a modern data pipeline alone will unlock possibilities and save the client time and money. Made Tech already has a track record of this and plenty of case studies to back it up. Once a conversation around data has begun, things can very quickly get really quite exciting. 

Machine Learning has an incredible range of problems it can solve, processes it can automate and insights it can generate. During these early conversations, it is a good idea to start talking to a member of the data team. You will quickly find that we are an excitable bunch who love talking about the art of the possibility and will very quickly have an idea of the potential the client can unlock. For now, let's begin with this; broadly speaking ML tasks are likely to fall into three basic catagories:

**forecasting**: Using historical data to predict future outcomes. How many passengers are likely to use public transport at this time or over that weekend? Given previous trends, how many patients should a hospital expect, when are the likely surges and what will demand be? Predicting staff requirements for shift allocation, estimating supply demands or forecasting spending patterns. These are all questions that can be tackled using regression modeling.

**Classification or recognition**: Identifying specific events based on input data. Using previous trends, can we predict which workers are likely to leave and why? Can we detect when an application to a service is fraudulent? Do we need to automatically track how many of a specific request type is submitted? Does the client need to filter spam from messaging services? Classification problems can even extend into include image recognition, automatically detecting contraband items in X-Ray images, checking for false identification documents at border crossings or extracting license plate information from cctv footage. Whenever you are looking to apply a specific label, be that to text, to images or to users, chances are you can train a classification model to do the work for you.

**Clustering**: Finally there is clustering. This is a version of unsupervised learning that can be used to determine or specify user groups based on selected metrics. Highlight similarities between teams, departments, organisations and locations, or even find outliers in a data set that might otherwise go unnoticed.

There are undeniably countless instances where the implementation of a ML model can deliver massive benefits to a department. The goal is to identify those opportunities and communicate to the client the huge benefits of doing so. Once the conversation has begun, a first step could even be brining a data scientist onboard to look at the data and begin building out a proof of concept. Unlocks in efficiency and improvements in reporting capabilities are massive wins for any department so it can very quickly become an exciting opportunity for a stake holder. Even if the data scientist isn't able to produce anything, they are likely to highlight gaps in infrastructure or flaws with their current data strategy, which in itself opens up entirely new conversations and possible work. It all begins with a conversation. 

## Data collection and Preparation
It may come as no surprise that core to any ML project is the data, given this is what is used to train a model and evaluate it effectiveness. The overall success of any machine learning project relies entirely on there being enough high quality, representative data available to analyse. Only from this can we hope to draw meaningful conclusions. As a rule of thumb, the bigger the raw data set the better. As the team proceed through numerous development and model training iterations the aim will be to produce a core of very high quality data, that will be segmented into training, validation and testing sets and fed to the model. Without a large initial data set, there is the risk that this process will reduce the data set to the point that the data is no longer representative of the business problem resulting in a poorly trained model that is unreliable. 

Considerations must always be made around the data collection process. When gathering potentially gigabyte's of data it is vital to ensure that it is correctly structured, schemas agreed upon, relationships drawn and documentation written. Thought must be given to whether the data source is reliable and if it truly represent whatever situations you wish to analyse? Is the data robust enough to account for changes in trends or unexpected events? If you are looking to perform time series analysis, does the data cover a long enough period of time? If you are looking to perform image recognition, do you have a large enough dataset of correctly labeled images? Data collection is a time intensive and resource heavy process and should not be rushed. A model can only ever be as good as the data it is trained on and there are very real moral and ethical implications to releasing a poorly trained model. 

Once the data has been colleted then we must consider where it is stored and how easily it can be accessed. Who has permission to view the data and what infrastructure is required to support it. Government data is often considered, at minimum, Official Sensitive, what processes are in place to ensure its security? Is there any personal data stored in the raw data? Does it need to be anonymised? Data at the scale required for effective machine learning can rarely be stored in simple excel sheets, building out a robust data infrastructure key. It is also a fantastic development for any business, and of course one that Made Tech has plenty of experience in delivering. The goal here is to move from single excel sheets held on an individuals computer, to a managed databases hosted on a suitable cloud platform that can be monitored, updated and maintained. 

During these early stages a data scientist can be bought in to consult on the data collection process and build out a proof of concept in a notebook to present to the client. This can usually be done with an excel extract and some time with the data. It must be stressed however that this is not a solution to a problem, it is part of the sales pitch. In order to build a robust ML solution, the client will likely need a to modernise their data storage solution. The building of this solution can be handed to data engineers (the unsung heros of any successful ML project) while the data scientists either continue to familierise themselves with the data or step back from the project to allow this first phase to be complete. 

## Exploratory Data Analysis and Visualisation
Once the data has been collected and made accessible, it is time begin the process of assessing its quality. Any modeling exercise requires as much very high quality data as possible. The team must assess whether they have gathered enough raw data, from which they can distill a gold standard training set, or whether the collection process needs to continue. This initial interrogation is referred to as exploratory data analysis (EDA). 

 Through this process, the data scientists will continue to cultivate a deeper understanding of the subject matter, now seen through the ever sharpening lens of quantitative data. They will uncover where data is missing, through poor reporting or collection, how various features interact with one another and will begin to build up a picture of how the data may fit into a model. They may even be able to begin to have as idea as to which models may suite the problem best. If they have been heavily involved in the data collection process then there is the potential that this will be a fairly quick process. If however they are coming to a data cold then, depending on the size of the data set, it could take a number of weeks for them to fully grasp the shape and quality of the data.  

Throughout this process, a vital tool in the data scientist arsenal is data visulisation. This serve two purposes. Firstly, as an excellent tool for intuitive analysis. It is far easier to identify relationships and correlations from a plot than it is from a column of numbers that will likely be several tens if not hundreds of thousands of rows long. The second role of visulisations, easily overlooked by the eager data scientist lost in the data, is one of communication. EDA does not often produce outputs in the same way as regular development, presenting meaningful plots and graphs to the client reassures them that work is being done and progress is being made. It will also likely be a spark for conversation as stake holders see, often for the first time, data that supports (or contradicts) their hard earned industry expertise. Encouraging this conversation is a great way to drive a project forward and build momentum and backing. With the right thought, even this early analysis can be seen as a massive win for the project and its stakeholder and provides a tangible results that can be paraded to the wider business. 

## Feature Engineering, Model Training and Model Evaluation

Feature engineering is a fundamental component of any machine learning project. It is the process of selecting, manipulating, extracting and otherwise transforming raw data into features that can be fed into a model and used in the training process. When done correctly, this process will simplify the model, reducing the time it takes to train, while still retaining all the information. Effective feature selection is essential in improving model accuracy, given however that there are no hard and fast rules for this process, it is often considered as much an art as it is a scientific process. 

Hand in hand with this is model selection, training and evaluation. Every machine learning problem has a plethora of different algorithms that may be used to solve them. It may be that the nature of the data and the trends it contains responds better or worse to a certain algorithmic approaches. The nature of ML means that often we will not know until we try. There are also hyperparameters for each model that must be tuned that will again have an effect on how well the model preforms. 

All of this leads to a very time consuming process, often requiring many iterations before a final feature set and model choice is made. It may even transpire that the situation is not completely described by the current data set and it is necessary to return to the data collection process and for the cycle to begin again. It is vital that throughout this process the client is kept well informed of what is going on. Sometimes the result of a feature engineering sprint will simply be reporting what doesn't work, important information that translates to progress, but something that can be frustrating for a client looking for tangible deliveries. 

## Business Goal Evaluation
Once a feature set and model has been developed and is producing results to an acceptable level of accuracy, it is important to evaluate how well that model performs using live data and how well it meets the business objectives. The implementation of any ML solution has the potential to have a massive impact on the operation of any department, freeing up recourses, delivering deeper insight and taking months off delivery time scales, however often the road to this impact is difficult to navigate. 

At all times, but particularly when considering the sector in which Made Tech operates, it is incredibly important to keep in mind the down stream consequences of our solutions. A model can only ever provides an outcome with a certain level of accuracy, there will always be outlier cases that the model is not able to predict. Put simply, it will not be correct 100% of the time so we must consider the impact of those errors. Does the model over estimate a figure for a report, who is seeing that report and what decisions are being made? If that model is classifying fraudulent benefit claims, does the model show bias? What impact will a false negative have on a persons life? 

These are all questions that must be asked and deeply considered when building a service. Made Tech has a responsibility to deliver the best solution to a problem. Sometimes that may mean developing exciting machine leaning pipelines, other times the situation may be better served by taking the time to fully map out the business logic and "simply" programming that into the pipeline instead. 


## Predictions and deployment 
Once the client is happy with the quality of the trained model, it is time to deploy to the production environment, begin feeding it real data and start having it produce actionable predictions. How and where the model is deployed depends on a number of things. How large is the model? Some models require huge sets of training data to be stored inorder for the model to run. Others require only a small number of parameters. 
What is an acceptable level of latency when returning predictions? Will they feed a report that is updated once a day/week/month or will they drive interactions on a website and require single digit millisecond response times. 
These are all questions that must be considered when developing the ml pipeline.  

Machine learning models require constant monitoring once they are live, monitoring should be built into your pipeline as a priority. As time goes on, trends change and demographics shift, there are plenty of reasons as to why the model you trained yesterday might not work a month, week or even day from now, this is expected and part of the ml life cycle. 


<!-- Where are the predictions being used
Are they being monitored for model drift with evolving live data / how are we monitoring/measuring reliability
How is this worked into a pipeline 
There is a difference between developing a model in a notebook and deploying a model to a live env
Heavy collaboration with data engineers
How does the client want to interact with the model 

MLmodels will require constant monitoring and development as the data will inevitably change shape over the course of time. 
Does the client have the requisite skills / developers in place to maintain a data pipeline / ML model
has up skilling been park of the delivery -->

## Hosting 

## Conclusions
If the client has data that they wish to be interrogated then there is likely something that a data scientist can do. This creates an incredible opportunity for Made Tech to step forward and offer to do more. Once a data scientist has been able to spend some time with the data, be that taken from years of excel spread sheets or scraped from the web, 